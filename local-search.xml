<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>File IO</title>
    <link href="/2021/02/02/File-IO/"/>
    <url>/2021/02/02/File-IO/</url>
    
    <content type="html"><![CDATA[<h1 id="Numpy-read-and-write"><a href="#Numpy-read-and-write" class="headerlink" title="Numpy read and write"></a>Numpy read and write</h1><h2 id="tofile-and-fromfile"><a href="#tofile-and-fromfile" class="headerlink" title="tofile() and fromfile()"></a>tofile() and fromfile()</h2><ul><li>tofile()将数组中的数据以二进制的格式写进文件</li><li>tofile()输出的数据不保存数组形状和元素类型等信息</li><li></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.arrange(<span class="hljs-number">0</span>, <span class="hljs-number">12</span>)<br>a.tofile(<span class="hljs-string">&quot;a.bin&quot;</span>)<br>b = np.fromfile(<span class="hljs-string">&quot;a.bin&quot;</span>, dtype = np.int32)<br></code></pre></td></tr></table></figure><h2 id="Reading-text-and-CSV-files"><a href="#Reading-text-and-CSV-files" class="headerlink" title="Reading text and CSV files"></a>Reading text and CSV files</h2><p>CSV： comma-separated values</p><h3 id="没有任何遗漏数据"><a href="#没有任何遗漏数据" class="headerlink" title="没有任何遗漏数据"></a>没有任何遗漏数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>tutorial</tag>
      
      <tag>python</tag>
      
      <tag>file io</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Useful Python</title>
    <link href="/2021/02/02/Useful-Python/"/>
    <url>/2021/02/02/Useful-Python/</url>
    
    <content type="html"><![CDATA[<h2 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h2><ul><li><code>str.split(str = &#39; &#39;, num = num)</code>, 传入str字符串作为分割字符，把str分割成num+1个字符串，并返回一个list</li><li><code>hasattr(object, name = &#39;name&#39;)</code>，传入对象，用于判断object钟是否包含对象属性name，返回bool</li></ul><h1 id="MMCV"><a href="#MMCV" class="headerlink" title="MMCV"></a>MMCV</h1><h2 id="1-1自定义扩展开发I-O"><a href="#1-1自定义扩展开发I-O" class="headerlink" title="1.1自定义扩展开发I/O"></a>1.1自定义扩展开发I/O</h2><h3 id="1-1-1"><a href="#1-1-1" class="headerlink" title="1.1.1"></a>1.1.1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@register_handle(<span class="hljs-params"><span class="hljs-string">&#x27;npy&#x27;</span></span>)</span><span class="hljs-comment">#装饰器，把实现的handler注册到mmcv钟，然后mmcv就可以直接找到该handler。</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NpyHandler</span>(<span class="hljs-params">BaseFileHandler</span>):</span><br>    <br>    <span class="hljs-comment"># np.save, np.load,  https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.load.html</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_from_fileobj</span>(<span class="hljs-params">self, file, **kwargs</span>):</span><br>        <span class="hljs-keyword">return</span> np.load(file)<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dump_to_fileobj</span>(<span class="hljs-params">self, obj, file, **kwargs</span>):</span><br>        np.save(file, obj)<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dump_to_str</span>(<span class="hljs-params">self, obj, **kwargs</span>):</span><br>        <span class="hljs-keyword">return</span> obj.tobytes()<br></code></pre></td></tr></table></figure><p>对外读写接口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#文件读取流程。</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load</span>(<span class="hljs-params">file, file_format = <span class="hljs-literal">None</span>, **kwargs</span>)</span><br><span class="hljs-function"><span class="hljs-title">return</span> <span class="hljs-title">obj</span></span><br><span class="hljs-function"></span><br><span class="hljs-function">#文件写流程</span><br><span class="hljs-function"><span class="hljs-title">def</span> <span class="hljs-title">dump</span>(<span class="hljs-params">obj, file = <span class="hljs-literal">None</span>, file_format = <span class="hljs-literal">None</span>, **kwargs</span>)</span><br><span class="hljs-function"></span><br><span class="hljs-function"><span class="hljs-title">import</span> <span class="hljs-title">mmcv</span></span><br>data = mmcv.load(&quot;test.json&quot;)<br>mmcv.dump(data, <span class="hljs-string">&quot;out.json&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>tutorial</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Frame-Recurrent Video Super-Resolution(CVPR 2018)</title>
    <link href="/2021/01/28/Frame-Recurrent-Video-Super-Resolution/"/>
    <url>/2021/01/28/Frame-Recurrent-Video-Super-Resolution/</url>
    
    <content type="html"><![CDATA[<p>In this paper, an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame is proposed.</p><a id="more"></a><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>作者提出了一种端到端的训练框架FRVSR，它使用前一个生成的HR估计帧加上当前的LR输入帧去估计下一帧的输出，有助于促进输出结果的时间连贯性，同时也减少了计算量。优点在于每帧都只被处理一次，并且采用循环框架，可以在更大时间范围内传播信息且不增加计算量。文章也进行了大量的实验，实验细节比较详尽，分析了不同网络设计的性能。</p><h2 id="Problem-Statement-and-Research-Objective"><a href="#Problem-Statement-and-Research-Objective" class="headerlink" title="Problem Statement and Research Objective"></a>Problem Statement and Research Objective</h2><p>The latest sota video super-resolution methods approach the problem by combining a batch of LR frames to estimate a single HR frame, but existing methods can’t reach a pleasing flickering artifacts to produce temporally consistent frames. To address the issues the authors exploit the temporal relationships in the input and improve reconstruction for video super-resolution by frame-recurrent framework.</p><h2 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a>Method(s)</h2><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><p>The author relates $$I^{LR}<em>{t-1}$$ and $I</em>{t}^{LR}$ with optical flow network $FNet$, the output $F_{t}^{LR}$ going through upsampling module to HR space $F_{t}^{HR}$. Then $F_{t}^{HR}$ is used to warp the $I_{t-1}^{est}$, and the warped outputs are transformed by space-to-depth transformation to LR space, namely $S_{s}(I_{t-1}^{est})$. Finally, using the concatenation of $I_{t}^{LR}$ and $S_{s}(I_{t-1}^{est})$ as input to feed the SRNet, reaching our objective $I_{t}^{est}$.</p><p><img src="https://github.com/xuboshen/xuboshen.github.io/tree/master/img/FRVSR.png" alt="FRVSR"></p><p>The final estimate $I_t^{est}$ of the framework is the output of the super-resolution network SRNet: $SRNet(I_t^{LR}\oplus S_s(WP(I_{t-1}^{est}\oplus UP(FNet(I_{t-1}^{LR}\oplus I_t^{LR})))))$.</p><h4 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h4><p>$\mathcal{L}_{sr} = ||I_t^{est} - I_t^{HR} ||_2^2$</p><p>$\mathcal{L}<em>{flow} = ||WP(I</em>{t - 1}^{LR}, F^{LR}) - I_t^{HR} ||_2^2$</p><p>由于没有实际的光流数据集，所以增加了一个辅助的loss帮助FNet生成更真实的光流。</p><p>$\mathcal{L} = \mathcal{L}<em>{flow} + \mathcal{L}</em>{sr}$</p><h4 id="FNet"><a href="#FNet" class="headerlink" title="FNet"></a>FNet</h4><p><img src="https://github.com/xuboshen/xuboshen.github.io/tree/master/img/FNet.png" alt="FNet"></p><h4 id="SRNet"><a href="#SRNet" class="headerlink" title="SRNet"></a>SRNet</h4><p><img src="https://github.com/xuboshen/xuboshen.github.io/tree/master/img/SRNet.png" alt="SRNet"></p><h4 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h4><ul><li>$\oplus$ denotes the concatenation of images in the channel dimension.</li><li>All convolutions in both networks uses 3×3 kernels with stride 1, except for the transposed convolutions in SRNet which use stride 2 for spatial <u>upsampling</u>. </li><li>The leaky ReLU units in FNet use a leakage factor of 0.2.</li><li>The notation 2× indicates that the corresponding block is duplicated. </li></ul><h4 id="Transposed-convolution"><a href="#Transposed-convolution" class="headerlink" title="Transposed convolution"></a>Transposed convolution</h4><p>Transposed convolution is one of upsampling methods(pixel shuffle can reduce the computational cost). It’s a special kind of convolution with zero paddings to expand the size of input image, then revolve the kernel and convolve again.</p><p>Tutorial: ://zhuanlan.zhihu.com/p/48501100</p><h4 id="Space-to-depth"><a href="#Space-to-depth" class="headerlink" title="Space-to-depth"></a>Space-to-depth</h4><p> $S_s:[0, 1]^{sH\times sW\times C}\rightarrow [0, 1]^{H\times W\times s^2C}$, mapping to LR space.</p><p>The operator can be formally described as $S_s (I)<em>{i,j,k} = I</em>{si+k% s, sj + (k/s)%s, k/s^2}$, with zero-based indexing, modulus % and integer division /.</p><p><img src="https://github.com/xuboshen/xuboshen.github.io/tree/master/img/space_to_depth.png" alt="space_to_depth"></p><p>PyTorch的实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">space_to_depth</span>(<span class="hljs-params">x, block_size</span>):</span><br>    n, c, h, w = x.size() <span class="hljs-comment"># 输入x只能为4维tensor的batch形式，否则warning</span><br>    unfolded_x = torch.nn.functional.unfold(x, block_size, stride=block_size)<br>    <span class="hljs-keyword">return</span> unfolded_x.view(n, c * block_size ** <span class="hljs-number">2</span>, h // block_size, w // block_size)<br></code></pre></td></tr></table></figure><h3 id="Based-methods"><a href="#Based-methods" class="headerlink" title="Based methods"></a>Based methods</h3><ul><li>Optical flow method.</li><li>Use bilinear interpolation as Warping module<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="M. Jaderberg, K. Simonyan, A. Zisserman, and K.Kavukcuoglu. Spatial transformer networks. In NIPS, 2015.">[1]</span></a></sup>, one of the innovative point.</li><li>Initialize the networks with the Xavier method<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.">[2]</span></a></sup>.</li><li>Train the model using Adam optimizer<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="D.Kingma and J.Ba. Adam: A method for stochastic optimization. In ICLR, 2015.">[3]</span></a></sup>.</li><li>Recurrent architecture for video super-resolution with shallow networks<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Y.Huang, W.Wang and L.Wang. Bidirectional recurrent convolutional networks for multi-frame super-resolution. In NIPS, 2015.">[4]</span></a></sup>.</li></ul><h2 id="Evaluation-and-justifications"><a href="#Evaluation-and-justifications" class="headerlink" title="Evaluation and justifications"></a>Evaluation and justifications</h2><ul><li>所有大的计算量都在LR space完成。All computationally intensive operations should be performed in LR space, which significantly reduce the computational cost</li><li>利用$F^{HR}$去Warp$I_{t-1}^{est}$。Having direct access to the previous output can help the network to produce a temporally consistent estimate for the following frame, thus warping the previous HR estimate and feed it to the super-resolution network.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>超越了sliding window的所有方法The FRVSR works more efficient than existing sliding window approaches. </li><li>在不同设置上都超出了baseline的表现The model outperforms competing baselines in various different settings. </li><li>The FRVSR, small model, produces results that are very close to the much larger model VSR 10-128 on the Vid4 dataset.</li><li>超过了VSR的sota方法The model also significantly outperforms sota video super-resolution approaches both quantitatively and qualitatively on a standard benchmark dataset.</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Other papars: C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In <em>ECCV</em>, 2014.</p><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sajjadi_Frame-Recurrent_Video_Super-Resolution_CVPR_2018_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2018/papers/Sajjadi_Frame-Recurrent_Video_Super-Resolution_CVPR_2018_paper.pdf</a></p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>M. Jaderberg, K. Simonyan, A. Zisserman, and K.Kavukcuoglu. Spatial transformer networks. In <em>NIPS</em>, 2015.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In <em>AISTATS</em>, 2010.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>D.Kingma and J.Ba. Adam: A method for stochastic optimization. In <em>ICLR</em>, 2015.<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Y.Huang, W.Wang and L.Wang. Bidirectional recurrent convolutional networks for multi-frame super-resolution. In <em>NIPS</em>, 2015.<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>虽然torch.nn.functional.pixel_shuffle 和tf.nn.depth_to_space的效果一模一样, PyTorch并没有能够实现tf.nn.space_to_depth的函数，不过我们可以通过torch.nn.functional.unfold去实现：<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://stackoverflow.com/questions/58857720/is-there-an-equivalent-pytorch-function-for-tf-nn-space-to-depth">https://stackoverflow.com/questions/58857720/is-there-an-equivalent-pytorch-function-for-tf-nn-space-to-depth</a>, 4:10 PM, 1/29/2021<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>Video Super-Resolution</tag>
      
      <tag>Frame-Recurrent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>anaconda useful commands</title>
    <link href="/2021/01/27/anaconda-useful-commands/"/>
    <url>/2021/01/27/anaconda-useful-commands/</url>
    
    <content type="html"><![CDATA[<h1 id="Basic-usage-of-conda"><a href="#Basic-usage-of-conda" class="headerlink" title="Basic usage of conda"></a>Basic usage of conda</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># managing conda</span><br>conda --version <span class="hljs-comment"># check the installation and version</span><br>conda update conda <span class="hljs-comment"># update the conda to the latest version</span><br><br><span class="hljs-comment"># managing environment and python</span><br>conda create -n name python=<span class="hljs-number">3.8</span><span class="hljs-comment"># create a new virtual environment</span><br>conda activate name   <span class="hljs-comment"># activate the environment</span><br>conda info --envs   <span class="hljs-comment"># to see a list of all your environments</span><br>conda activate       <span class="hljs-comment"># return to base</span><br>python --version   <span class="hljs-comment"># to check the version of python</span><br><br><span class="hljs-comment"># managing packages</span><br>conda search torch   <span class="hljs-comment"># check to see if a package you have not installed named &quot;torch&quot; is available from the Anaconda repository (online)</span><br>conda install pandas   <span class="hljs-comment"># install a certain package</span><br>conda <span class="hljs-built_in">list</span>  <span class="hljs-comment"># list all packages installed in the environment</span><br><br><br></code></pre></td></tr></table></figure><p>For the packages not in Anaconda, we can simply use the pip command to install them.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/01/17/hello-world/"/>
    <url>/2021/01/17/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><p>Notice: <code>hexo clean</code> before <code>hexo g or hexo s</code></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h3 id="Set-abstract"><a href="#Set-abstract" class="headerlink" title="Set abstract"></a>Set abstract</h3><p><code>正文的一部分作为摘要&lt;!-- more --&gt;余下的正文</code></p><h3 id="Add-footnotes"><a href="#Add-footnotes" class="headerlink" title="Add footnotes"></a>Add footnotes</h3><p><code>This is a sentence&lt;sup id=&quot;fnref:1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn:1&quot; rel=&quot;footnote&quot;&gt;&lt;span class=&quot;hint--top hint--rounded&quot; aria-label=&quot;This is the corresponding footnote.</code><br>“&gt;[1]</span></a></sup>         <section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>This is the corresponding footnote.```<br><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section></p>]]></content>
    
    
    
    <tags>
      
      <tag>Tutorial</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
